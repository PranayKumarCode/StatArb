{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import refinitiv.data as rd\n",
    "from refinitiv.data.content import historical_pricing as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994f508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alldata.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(type(data))\n",
    "print(data.keys())\n",
    "\n",
    "name_list = data['name_list']\n",
    "industry_list = data['industry_list']\n",
    "index_list = data['index_list']\n",
    "cusip_list = data['cusip_list']\n",
    "universe = data['universe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dcae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.open_session()\n",
    "\n",
    "ric = \"ABI.BR\"\n",
    "start_date = \"2019-09-01\"\n",
    "end_date = \"2025-08-01\"\n",
    "\n",
    "price_df = rd.get_history(\n",
    "    universe=ric,\n",
    "    start=start_date,\n",
    "    end=end_date,\n",
    "    interval=\"daily\"\n",
    ")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'price': price_df['TRDPRC_1'],\n",
    "    'volume_shares': price_df['ACVOL_UNS'],\n",
    "    'bid': price_df['BID'],\n",
    "    'ask': price_df['ASK']\n",
    "})\n",
    "\n",
    "fundamental_response = rd.get_data(\n",
    "    universe=ric,\n",
    "    fields=[\n",
    "        \"TR.TotalReturn.Date\",\n",
    "        \"TR.TotalReturn\",           # Total Return Index\n",
    "        \"TR.PriceToBook\",           # Price to Book (Market-to-Book)\n",
    "        \"TR.CompanyMarketCap\"       # Market Cap\n",
    "    ],\n",
    "    parameters={\n",
    "        \"SDate\": start_date,\n",
    "        \"EDate\": end_date,\n",
    "        \"Frq\": \"D\",\n",
    "        \"Curn\": \"EUR\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nFundamental data response:\")\n",
    "print(fundamental_response.head(10))\n",
    "print(f\"\\nColumns: {fundamental_response.columns.tolist()}\")\n",
    "\n",
    "if fundamental_response is not None and isinstance(fundamental_response, pd.DataFrame):\n",
    "    fund_df = fundamental_response.copy()\n",
    "    \n",
    "    if 'Date' in fund_df.columns:\n",
    "        fund_df['Date'] = pd.to_datetime(fund_df['Date'])\n",
    "        fund_df.set_index('Date', inplace=True)\n",
    "        \n",
    "        if 'Instrument' in fund_df.columns:\n",
    "            fund_df.drop('Instrument', axis=1, inplace=True)\n",
    "        \n",
    "        print(f\"\\nFundamental data with Date index:\")\n",
    "        print(fund_df.head(10))\n",
    "        \n",
    "        df = df.join(fund_df, how='left')\n",
    "    else:\n",
    "        print(\"\\nWARNING: No Date column found in fundamental data!\")\n",
    "        print(\"Available columns:\", fund_df.columns.tolist())\n",
    "\n",
    "column_mapping = {\n",
    "    'Total Return': 'tri',\n",
    "    'Price To Book Value': 'mtbv',\n",
    "    'Company Market Cap': 'cap'\n",
    "}\n",
    "\n",
    "for old_name, new_name in column_mapping.items():\n",
    "    if old_name in df.columns:\n",
    "        df.rename(columns={old_name: new_name}, inplace=True)\n",
    "\n",
    "df['volume'] = df['volume_shares'] * df['price']\n",
    "df.drop('volume_shares', axis=1, inplace=True)\n",
    "\n",
    "if 'mtbv' in df.columns:\n",
    "    df['mtbv'] = df['mtbv'].ffill()\n",
    "if 'cap' in df.columns:\n",
    "    df['cap'] = df['cap'].ffill()\n",
    "\n",
    "desired_order = ['price', 'tri', 'volume', 'mtbv', 'cap', 'bid', 'ask']\n",
    "existing_cols = [col for col in desired_order if col in df.columns]\n",
    "df = df[existing_cols]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(df.head(15))\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nNull counts:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nSample with non-null data:\")\n",
    "print(df[df['tri'].notna()].head(10))\n",
    "\n",
    "df.to_csv(\"ABI_BR_Data.csv\", index=True)\n",
    "print(f\"\\nData saved to ABI_BR_Data.csv\")\n",
    "\n",
    "rd.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2bb338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8b9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "rd.open_session()\n",
    "\n",
    "unique_rics = universe['RIC'].unique()\n",
    "print(f\"Total rows in universe: {len(universe)}\")\n",
    "print(f\"Unique RICs to process: {len(unique_rics)}\")\n",
    "\n",
    "start_date = \"2019-09-01\"\n",
    "end_date = \"2025-08-01\"\n",
    "\n",
    "all_data = []\n",
    "failed_rics = []\n",
    "\n",
    "total_rics = len(unique_rics)\n",
    "print(f\"\\nProcessing {total_rics} unique stocks...\")\n",
    "\n",
    "for idx, ric in enumerate(unique_rics, 1):\n",
    "    print(f\"[{idx}/{total_rics}] Processing {ric}...\", end=' ')\n",
    "    \n",
    "    try:\n",
    "        # --- Fetch historical PRICING data ---\n",
    "        price_df = rd.get_history(\n",
    "            universe=ric,\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "            interval=\"daily\"\n",
    "        )\n",
    "        \n",
    "        # Extract only what we need\n",
    "        df = pd.DataFrame({\n",
    "            'RIC': ric,  # Add RIC column\n",
    "            'price': price_df['TRDPRC_1'],\n",
    "            'volume_shares': price_df['ACVOL_UNS'],\n",
    "            'bid': price_df['BID'],\n",
    "            'ask': price_df['ASK']\n",
    "        })\n",
    "        \n",
    "        # --- Fetch FUNDAMENTAL data WITH DATES ---\n",
    "        fundamental_response = rd.get_data(\n",
    "            universe=ric,\n",
    "            fields=[\n",
    "                \"TR.TotalReturn.Date\",\n",
    "                \"TR.TotalReturn\",\n",
    "                \"TR.PriceToBook\",\n",
    "                \"TR.CompanyMarketCap\"\n",
    "            ],\n",
    "            parameters={\n",
    "                \"SDate\": start_date,\n",
    "                \"EDate\": end_date,\n",
    "                \"Frq\": \"D\",\n",
    "                \"Curn\": \"EUR\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Process fundamental data\n",
    "        if fundamental_response is not None and isinstance(fundamental_response, pd.DataFrame):\n",
    "            fund_df = fundamental_response.copy()\n",
    "            \n",
    "            if 'Date' in fund_df.columns:\n",
    "                fund_df['Date'] = pd.to_datetime(fund_df['Date'])\n",
    "                fund_df.set_index('Date', inplace=True)\n",
    "                \n",
    "                if 'Instrument' in fund_df.columns:\n",
    "                    fund_df.drop('Instrument', axis=1, inplace=True)\n",
    "                \n",
    "                # Merge with price data\n",
    "                df = df.join(fund_df, how='left')\n",
    "        \n",
    "        # Rename columns\n",
    "        column_mapping = {\n",
    "            'Total Return': 'tri',\n",
    "            'Price To Book Value': 'mtbv',\n",
    "            'Company Market Cap': 'cap'\n",
    "        }\n",
    "        \n",
    "        for old_name, new_name in column_mapping.items():\n",
    "            if old_name in df.columns:\n",
    "                df.rename(columns={old_name: new_name}, inplace=True)\n",
    "        \n",
    "        # Compute volume in EUR\n",
    "        df['volume'] = df['volume_shares'] * df['price']\n",
    "        df.drop('volume_shares', axis=1, inplace=True)\n",
    "        \n",
    "        # Forward-fill mtbv and cap\n",
    "        if 'mtbv' in df.columns:\n",
    "            df['mtbv'] = df['mtbv'].ffill()\n",
    "        if 'cap' in df.columns:\n",
    "            df['cap'] = df['cap'].ffill()\n",
    "        \n",
    "        # Reset index to make Date a column\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={'index': 'Date'}, inplace=True)\n",
    "        \n",
    "        # Reorder columns\n",
    "        desired_order = ['RIC', 'Date', 'price', 'tri', 'volume', 'mtbv', 'cap', 'bid', 'ask']\n",
    "        existing_cols = [col for col in desired_order if col in df.columns]\n",
    "        df = df[existing_cols]\n",
    "        \n",
    "        # Append to list\n",
    "        all_data.append(df)\n",
    "        \n",
    "        print(f\"✓ {len(df)} rows\")\n",
    "        \n",
    "        # Rate limiting - sleep briefly to avoid overwhelming the API\n",
    "        if idx % 10 == 0:\n",
    "            time.sleep(2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed: {e}\")\n",
    "        failed_rics.append(ric)\n",
    "        continue\n",
    "\n",
    "# --- 6. Combine all data ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Successfully processed: {len(all_data)} stocks\")\n",
    "print(f\"Failed: {len(failed_rics)} stocks\")\n",
    "\n",
    "if failed_rics:\n",
    "    print(f\"\\nFailed RICs:\")\n",
    "    for ric in failed_rics:\n",
    "        print(f\"  - {ric}\")\n",
    "\n",
    "# --- 7. Create combined DataFrame ---\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nCombined dataset shape: {combined_df.shape}\")\n",
    "    print(f\"Date range: {combined_df['Date'].min()} to {combined_df['Date'].max()}\")\n",
    "    print(f\"Unique stocks: {combined_df['RIC'].nunique()}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample data:\")\n",
    "    print(combined_df.head(10))\n",
    "    \n",
    "    # Save to CSV\n",
    "    combined_df.to_csv(\"all_stocks_data.csv\", index=False)\n",
    "    print(f\"\\n✓ Saved to all_stocks_data.csv\")\n",
    "    \n",
    "    # Show null counts\n",
    "    print(\"\\nNull counts by column:\")\n",
    "    print(combined_df.isnull().sum())\n",
    "else:\n",
    "    print(\"\\nNo data retrieved!\")\n",
    "\n",
    "# --- 8. Close session ---\n",
    "rd.close_session()\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b0c85a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RIC          0\n",
       "Date         0\n",
       "price     1430\n",
       "tri        396\n",
       "volume     474\n",
       "cap        881\n",
       "bid        513\n",
       "ask        590\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"all_stocks_data.csv\").isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a4bbff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4d32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "14ac7511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in universe: 1962\n",
      "Unique RICs to process: 411\n",
      "\n",
      "Processing 411 unique stocks...\n",
      "[1/411] Processing 1COVG.DE... ✓ 1506 rows\n",
      "[2/411] Processing AALB.AS... ✓ 1517 rows\n",
      "[3/411] Processing ABB.ST... ✓ 1488 rows\n",
      "[4/411] Processing ABI.BR... ✓ 1517 rows\n",
      "[5/411] Processing ABNd.AS... ✓ 1517 rows\n",
      "[6/411] Processing ACCP.PA... ✓ 1517 rows\n",
      "[7/411] Processing ACKB.BR... ✓ 1517 rows\n",
      "[8/411] Processing AD.AS... ✓ 1517 rows\n",
      "[9/411] Processing ADP.PA... ✓ 1517 rows\n",
      "[10/411] Processing ADSGn.DE... ✓ 1506 rows\n",
      "[11/411] Processing ADYEN.AS... ✓ 1517 rows\n",
      "[12/411] Processing AEGN.AS... ✓ 1517 rows\n",
      "[13/411] Processing AGES.BR... ✓ 1517 rows\n",
      "[14/411] Processing AIR.PA... ✓ 1517 rows\n",
      "[15/411] Processing AIRF.PA... ✓ 1517 rows\n",
      "[16/411] Processing AIRP.PA... ✓ 1517 rows\n",
      "[17/411] Processing AKE.PA... ✓ 1517 rows\n",
      "[18/411] Processing AKER.OL... ✓ 1487 rows\n",
      "[19/411] Processing AKRBP.OL... ✓ 1487 rows\n",
      "[20/411] Processing AKZO.AS... ✓ 1517 rows\n",
      "[21/411] Processing ALFA.ST... ✓ 1488 rows\n",
      "[22/411] Processing ALIVsdb.ST... ✓ 1488 rows\n",
      "[23/411] Processing ALSO.PA... ✓ 1517 rows\n",
      "[24/411] Processing ALTT.PA^D20... ✓ 157 rows\n",
      "[25/411] Processing ALVG.DE... ✓ 1506 rows\n",
      "[26/411] Processing AM.PA... ✓ 1517 rows\n",
      "[27/411] Processing AMBUb.CO... ✓ 1481 rows\n",
      "[28/411] Processing AMG.AS... ✓ 1517 rows\n",
      "[29/411] Processing AMUN.PA... ✓ 1517 rows\n",
      "[30/411] Processing APAM.AS... ✓ 1517 rows\n",
      "[31/411] Processing ARDS.AS... ✓ 1517 rows\n",
      "[32/411] Processing ARGX.BR... ✓ 1517 rows\n",
      "[33/411] Processing ASMI.AS... ✓ 1517 rows\n",
      "[34/411] Processing ASML.AS... ✓ 1517 rows\n",
      "[35/411] Processing ASRNL.AS... ✓ 1517 rows\n",
      "[36/411] Processing ASSAb.ST... ✓ 1488 rows\n",
      "[37/411] Processing ATCA.AS^A21... ✓ 362 rows\n",
      "[38/411] Processing ATCOa.ST... ✓ 1488 rows\n",
      "[39/411] Processing ATCOb.ST... ✓ 1488 rows\n",
      "[40/411] Processing ATOS.PA... ✓ 1517 rows\n",
      "[41/411] Processing AXAF.PA... ✓ 1517 rows\n",
      "[42/411] Processing AYV.PA... ✓ 1517 rows\n",
      "[43/411] Processing AZN.ST... ✓ 1488 rows\n",
      "[44/411] Processing BAKKA.OL... ✓ 1487 rows\n",
      "[45/411] Processing BAMN.AS... ✓ 1517 rows\n",
      "[46/411] Processing BAR.BR... ✓ 1517 rows\n",
      "[47/411] Processing BASFn.DE... ✓ 1506 rows\n",
      "[48/411] Processing BAYGn.DE... ✓ 1506 rows\n",
      "[49/411] Processing BEIG.DE... ✓ 1506 rows\n",
      "[50/411] Processing BESI.AS... ✓ 1517 rows\n",
      "[51/411] Processing BFIT.AS... ✓ 1517 rows\n",
      "[52/411] Processing BICP.PA... ✓ 1517 rows\n",
      "[53/411] Processing BIOX.PA... ✓ 1517 rows\n",
      "[54/411] Processing BMWG.DE... ✓ 1506 rows\n",
      "[55/411] Processing BNPP.PA... ✓ 1517 rows\n",
      "[56/411] Processing BOL.ST... ✓ 1488 rows\n",
      "[57/411] Processing BOLL.PA... ✓ 1517 rows\n",
      "[58/411] Processing BOSN.AS^K22... ✓ 822 rows\n",
      "[59/411] Processing BOUY.PA... ✓ 1517 rows\n",
      "[60/411] Processing BVI.PA... ✓ 1517 rows\n",
      "[61/411] Processing BWLPG.OL... ✓ 1487 rows\n",
      "[62/411] Processing BWO.OL... ✓ 1487 rows\n",
      "[63/411] Processing CAGR.PA... ✓ 1517 rows\n",
      "[64/411] Processing CAPP.PA... ✓ 1517 rows\n",
      "[65/411] Processing CARLb.CO... ✓ 1481 rows\n",
      "[66/411] Processing CARR.PA... ✓ 1517 rows\n",
      "[67/411] Processing CASP.PA... ✓ 1517 rows\n",
      "[68/411] Processing CHRH.CO^A24... ✓ 1107 rows\n",
      "[69/411] Processing CLARI.PA... ✓ 1517 rows\n",
      "[70/411] Processing CNAT.PA^G21... ✓ 485 rows\n",
      "[71/411] Processing CNPP.PA^F22... ✓ 720 rows\n",
      "[72/411] Processing COFA.PA... ✓ 1517 rows\n",
      "[73/411] Processing COFB.BR... ✓ 1517 rows\n",
      "[74/411] Processing COLOb.CO... ✓ 1481 rows\n",
      "[75/411] Processing COLR.BR... ✓ 1517 rows\n",
      "[76/411] Processing CONG.DE... ✓ 1506 rows\n",
      "[77/411] Processing CORB.AS... ✓ 1517 rows\n",
      "[78/411] Processing CVO.PA... ✓ 1517 rows\n",
      "[79/411] Processing DANO.PA... ✓ 1517 rows\n",
      "[80/411] Processing DANSKE.CO... ✓ 1481 rows\n",
      "[81/411] Processing DAST.PA... ✓ 1517 rows\n",
      "[82/411] Processing DB1Gn.DE... ✓ 1506 rows\n",
      "[83/411] Processing DBKGn.DE... ✓ 1506 rows\n",
      "[84/411] Processing DBV.PA... ✓ 1517 rows\n",
      "[85/411] Processing DEMANT.CO... ✓ 1481 rows\n",
      "[86/411] Processing DHLn.DE... ✓ 1506 rows\n",
      "[87/411] Processing DNB.OL... ✓ 1487 rows\n",
      "[88/411] Processing DNO.OL... ✓ 1487 rows\n",
      "[89/411] Processing DSMN.AS^E23... ✓ 961 rows\n",
      "[90/411] Processing DSV.CO... ✓ 1481 rows\n",
      "[91/411] Processing DTEGn.DE... ✓ 1506 rows\n",
      "[92/411] Processing ECMPA.AS... ✓ 1517 rows\n",
      "[93/411] Processing EDEN.PA... ✓ 1517 rows\n",
      "[94/411] Processing EDF.PA^F23... ✓ 968 rows\n",
      "[95/411] Processing ELIOR.PA... ✓ 1517 rows\n",
      "[96/411] Processing ELIS.PA... ✓ 1517 rows\n",
      "[97/411] Processing ELISA.HE... ✓ 1487 rows\n",
      "[98/411] Processing ELUXb.ST... ✓ 1488 rows\n",
      "[99/411] Processing EMEIS.PA... ✓ 1517 rows\n",
      "[100/411] Processing ENGIE.PA... ✓ 1517 rows\n",
      "[101/411] Processing ENX.PA... ✓ 1517 rows\n",
      "[102/411] Processing EONGn.DE... ✓ 1506 rows\n",
      "[103/411] Processing EQNR.OL... ✓ 1487 rows\n",
      "[104/411] Processing ERICb.ST... ✓ 1488 rows\n",
      "[105/411] Processing ERMT.PA... ✓ 1517 rows\n",
      "[106/411] Processing ESLX.PA... ✓ 1517 rows\n",
      "[107/411] Processing ESSITYb.ST... ✓ 1488 rows\n",
      "[108/411] Processing ETL.PA... ✓ 1517 rows\n",
      "[109/411] Processing EUCAR.PA^G22... ✓ 737 rows\n",
      "[110/411] Processing EUFI.PA... ✓ 1517 rows\n",
      "[111/411] Processing EURA.PA... ✓ 1517 rows\n",
      "[112/411] Processing EXHO.PA... ✓ 1517 rows\n",
      "[113/411] Processing FAGRO.BR... ✓ 1517 rows\n",
      "[114/411] Processing FLOW.AS... ✓ 1517 rows\n",
      "[115/411] Processing FMEG.DE... ✓ 1506 rows\n",
      "[116/411] Processing FNAC.PA... ✓ 1517 rows\n",
      "[117/411] Processing FORTUM.HE... ✓ 1487 rows\n",
      "[118/411] Processing FOUG.PA... ✓ 1517 rows\n",
      "[119/411] Processing FREG.DE... ✓ 1506 rows\n",
      "[120/411] Processing FRO.OL... ✓ 1487 rows\n",
      "[121/411] Processing FRVIA.PA... ✓ 1517 rows\n",
      "[122/411] Processing FTI.PA^B22... ✓ 634 rows\n",
      "[123/411] Processing FUGR.AS... ✓ 1517 rows\n",
      "[124/411] Processing GBLB.BR... ✓ 1517 rows\n",
      "[125/411] Processing GETIb.ST... ✓ 1488 rows\n",
      "[126/411] Processing GETP.PA... ✓ 1517 rows\n",
      "[127/411] Processing GFCP.PA... ✓ 1517 rows\n",
      "[128/411] Processing GJFG.OL... ✓ 1487 rows\n",
      "[129/411] Processing GLPG.AS... ✓ 1517 rows\n",
      "[130/411] Processing GMAB.CO... ✓ 1481 rows\n",
      "[131/411] Processing GN.CO... ✓ 1481 rows\n",
      "[132/411] Processing GNFT.PA... ✓ 1517 rows\n",
      "[133/411] Processing GOGL.OL^H25... ✓ 1487 rows\n",
      "[134/411] Processing GTT.PA... ✓ 1517 rows\n",
      "[135/411] Processing GVNV.AS^A22... ✓ 607 rows\n",
      "[136/411] Processing HEIG.DE... ✓ 1506 rows\n",
      "[137/411] Processing HEIN.AS... ✓ 1517 rows\n",
      "[138/411] Processing HEXAb.ST... ✓ 1488 rows\n",
      "[139/411] Processing HIAB.HE... ✓ 1487 rows\n",
      "[140/411] Processing HMb.ST... ✓ 1488 rows\n",
      "[141/411] Processing HNKG_p.DE... ✓ 1506 rows\n",
      "[142/411] Processing HRMS.PA... ✓ 1517 rows\n",
      "[143/411] Processing HUH1V.HE... ✓ 1487 rows\n",
      "[144/411] Processing ICAD.PA... ✓ 1517 rows\n",
      "[145/411] Processing IFXGn.DE... ✓ 1506 rows\n",
      "[146/411] Processing ILD.PA^J21... ✓ 546 rows\n",
      "[147/411] Processing IMCD.AS... ✓ 1517 rows\n",
      "[148/411] Processing IMTP.PA... ✓ 1517 rows\n",
      "[149/411] Processing INGA.AS... ✓ 1517 rows\n",
      "[150/411] Processing INGC.PA^K20... ✓ 316 rows\n",
      "[151/411] Processing INTER.AS^L22... ✓ 855 rows\n",
      "[152/411] Processing INVEb.ST... ✓ 1488 rows\n",
      "[153/411] Processing IPN.PA... ✓ 1517 rows\n",
      "[154/411] Processing ISOS.PA... ✓ 1517 rows\n",
      "[155/411] Processing ISS.CO... ✓ 1481 rows\n",
      "[156/411] Processing JCDX.PA... ✓ 1517 rows\n",
      "[157/411] Processing KBC.BR... ✓ 1517 rows\n",
      "[158/411] Processing KCRA.HE... ✓ 1487 rows\n",
      "[159/411] Processing KEMIRA.HE... ✓ 1487 rows\n",
      "[160/411] Processing KESKOB.HE... ✓ 1487 rows\n",
      "[161/411] Processing KINVb.ST... ✓ 1488 rows\n",
      "[162/411] Processing KNEBV.HE... ✓ 1487 rows\n",
      "[163/411] Processing KPN.AS... ✓ 1517 rows\n",
      "[164/411] Processing LAGA.PA... ✓ 1517 rows\n",
      "[165/411] Processing LEGD.PA... ✓ 1517 rows\n",
      "[166/411] Processing LHAG.DE... ✓ 1506 rows\n",
      "[167/411] Processing LIGHT.AS... ✓ 1517 rows\n",
      "[168/411] Processing LIN.DE... ✓ 1506 rows\n",
      "[169/411] Processing LOIM.PA... ✓ 1517 rows\n",
      "[170/411] Processing LSG.OL... ✓ 1487 rows\n",
      "[171/411] Processing LTEN.PA... ✓ 1517 rows\n",
      "[172/411] Processing LUN.CO^F22... ✓ 691 rows\n",
      "[173/411] Processing LVMH.PA... ✓ 1517 rows\n",
      "[174/411] Processing MAERSKb.CO... ✓ 1481 rows\n",
      "[175/411] Processing MBGn.DE... ✓ 1506 rows\n",
      "[176/411] Processing MDM.PA... ✓ 1517 rows\n",
      "[177/411] Processing MERY.PA... ✓ 1517 rows\n",
      "[178/411] Processing METSB.HE... ✓ 1487 rows\n",
      "[179/411] Processing METSO.HE... ✓ 1487 rows\n",
      "[180/411] Processing MICP.PA... ✓ 1517 rows\n",
      "[181/411] Processing MMTP.PA... ✓ 1517 rows\n",
      "[182/411] Processing MOWI.OL... ✓ 1487 rows\n",
      "[183/411] Processing MRCG.DE... ✓ 1506 rows\n",
      "[184/411] Processing MT.AS... ✓ 1517 rows\n",
      "[185/411] Processing MTXGn.DE... ✓ 1506 rows\n",
      "[186/411] Processing MUVGn.DE... ✓ 1506 rows\n",
      "[187/411] Processing MWDP.PA... ✓ 1517 rows\n",
      "[188/411] Processing NAS.OL... ✓ 1487 rows\n",
      "[189/411] Processing NDAFI.HE... ✓ 1487 rows\n",
      "[190/411] Processing NDASE.ST... ✓ 1488 rows\n",
      "[191/411] Processing NEL.OL... ✓ 1487 rows\n",
      "[192/411] Processing NELES.HE^D22... ✓ 651 rows\n",
      "[193/411] Processing NESTE.HE... ✓ 1487 rows\n",
      "[194/411] Processing NEXI.PA... ✓ 1517 rows\n",
      "[195/411] Processing NEXS.PA... ✓ 1517 rows\n",
      "[196/411] Processing NHY.OL... ✓ 1487 rows\n",
      "[197/411] Processing NN.AS... ✓ 1517 rows\n",
      "[198/411] Processing NOKIA.HE... ✓ 1487 rows\n",
      "[199/411] Processing NOVOb.CO... ✓ 1481 rows\n",
      "[200/411] Processing NSISb.CO... ✓ 1481 rows\n",
      "[201/411] Processing OCI.AS... ✓ 1517 rows\n",
      "[202/411] Processing ONTEX.BR... ✓ 1517 rows\n",
      "[203/411] Processing OPM.PA... ✓ 1517 rows\n",
      "[204/411] Processing ORAN.PA... ✓ 1517 rows\n",
      "[205/411] Processing OREP.PA... ✓ 1517 rows\n",
      "[206/411] Processing ORK.OL... ✓ 1487 rows\n",
      "[207/411] Processing ORNBV.HE... ✓ 1487 rows\n",
      "[208/411] Processing ORSTED.CO... ✓ 1481 rows\n",
      "[209/411] Processing OUT1V.HE... ✓ 1487 rows\n",
      "[210/411] Processing PERP.PA... ✓ 1517 rows\n",
      "[211/411] Processing PEUP.PA^A21... ✓ 355 rows\n",
      "[212/411] Processing PHG.AS... ✓ 1517 rows\n",
      "[213/411] Processing PNDORA.CO... ✓ 1481 rows\n",
      "[214/411] Processing PROX.BR... ✓ 1517 rows\n",
      "[215/411] Processing PRTP.PA... ✓ 1517 rows\n",
      "[216/411] Processing PRX.AS... ✓ 1511 rows\n",
      "[217/411] Processing PTNL.AS... ✓ 1517 rows\n",
      "[218/411] Processing PUBP.PA... ✓ 1517 rows\n",
      "[219/411] Processing QDT.PA... ✓ 1517 rows\n",
      "[220/411] Processing RAND.AS... ✓ 1517 rows\n",
      "[221/411] Processing RBREW.CO... ✓ 1481 rows\n",
      "[222/411] Processing RCOP.PA... ✓ 1517 rows\n",
      "[223/411] Processing REL.AS... ✓ 1517 rows\n",
      "[224/411] Processing RENA.PA... ✓ 1517 rows\n",
      "[225/411] Processing RUBF.PA... ✓ 1517 rows\n",
      "[226/411] Processing RWEG.DE... ✓ 1506 rows\n",
      "[227/411] Processing RXL.PA... ✓ 1517 rows\n",
      "[228/411] Processing SAF.PA... ✓ 1517 rows\n",
      "[229/411] Processing SALM.OL... ✓ 1487 rows\n",
      "[230/411] Processing SAMPO.HE... ✓ 1487 rows\n",
      "[231/411] Processing SAND.ST... ✓ 1488 rows\n",
      "[232/411] Processing SAPG.DE... ✓ 1506 rows\n",
      "[233/411] Processing SASY.PA... ✓ 1517 rows\n",
      "[234/411] Processing SBMO.AS... ✓ 1517 rows\n",
      "[235/411] Processing SCAb.ST... ✓ 1488 rows\n",
      "[236/411] Processing SCHN.PA... ✓ 1517 rows\n",
      "[237/411] Processing SCOR.PA... ✓ 1517 rows\n",
      "[238/411] Processing SEBF.PA... ✓ 1517 rows\n",
      "[239/411] Processing SEBa.ST... ✓ 1488 rows\n",
      "[240/411] Processing SECUb.ST... ✓ 1488 rows\n",
      "[241/411] Processing SESFd.PA... ✓ 1517 rows\n",
      "[242/411] Processing SEVI.PA^B22... ✓ 636 rows\n",
      "[243/411] Processing SGEF.PA... ✓ 1517 rows\n",
      "[244/411] Processing SGOB.PA... ✓ 1517 rows\n",
      "[245/411] Processing SHBa.ST... ✓ 1488 rows\n",
      "[246/411] Processing SHEL.AS... ✓ 1517 rows\n",
      "[247/411] Processing SIEGn.DE... ✓ 1506 rows\n",
      "[248/411] Processing SIM.CO^J23... ✓ 1045 rows\n",
      "[249/411] Processing SKAb.ST... ✓ 1488 rows\n",
      "[250/411] Processing SKFb.ST... ✓ 1488 rows\n",
      "[251/411] Processing SOF.BR... ✓ 1517 rows\n",
      "[252/411] Processing SOGN.PA... ✓ 1517 rows\n",
      "[253/411] Processing SOIT.PA... ✓ 1517 rows\n",
      "[254/411] Processing SOLB.BR... ✓ 1517 rows\n",
      "[255/411] Processing SOPR.PA... ✓ 1517 rows\n",
      "[256/411] Processing SPIE.PA... ✓ 1517 rows\n",
      "[257/411] Processing SSABa.ST... ✓ 1488 rows\n",
      "[258/411] Processing STB.OL... ✓ 1487 rows\n",
      "[259/411] Processing STDM.PA... ✓ 1517 rows\n",
      "[260/411] Processing STERV.HE... ✓ 1487 rows\n",
      "[261/411] Processing STMPA.PA... ✓ 1517 rows\n",
      "[262/411] Processing SUBC.OL... ✓ 1487 rows\n",
      "[263/411] Processing SWEDa.ST... ✓ 1488 rows\n",
      "[264/411] Processing SWMA.ST^A23... ✓ 843 rows\n",
      "[265/411] Processing TCFP.PA... ✓ 1517 rows\n",
      "[266/411] Processing TEL.OL... ✓ 1487 rows\n",
      "[267/411] Processing TEL2b.ST... ✓ 1488 rows\n",
      "[268/411] Processing TELIA.ST... ✓ 1488 rows\n",
      "[269/411] Processing TELIA1.HE... ✓ 1487 rows\n",
      "[270/411] Processing TEPRF.PA... ✓ 1517 rows\n",
      "[271/411] Processing TFFP.PA... ✓ 1517 rows\n",
      "[272/411] Processing TGS.OL... ✓ 1487 rows\n",
      "[273/411] Processing TIETO.HE... ✓ 1487 rows\n",
      "[274/411] Processing TKTT.PA... ✓ 1517 rows\n",
      "[275/411] Processing TKWY.AS... ✓ 1517 rows\n",
      "[276/411] Processing TNET.BR^J23... ✓ 1060 rows\n",
      "[277/411] Processing TOM.OL... ✓ 1487 rows\n",
      "[278/411] Processing TRIA.PA... ✓ 1517 rows\n",
      "[279/411] Processing TRYG.CO... ✓ 1481 rows\n",
      "[280/411] Processing TTEF.PA... ✓ 1517 rows\n",
      "[281/411] Processing TWKNc.AS... ✓ 1517 rows\n",
      "[282/411] Processing TYRES.HE... ✓ 1487 rows\n",
      "[283/411] Processing UBIP.PA... ✓ 1517 rows\n",
      "[284/411] Processing UCB.BR... ✓ 1517 rows\n",
      "[285/411] Processing UMI.BR... ✓ 1517 rows\n",
      "[286/411] Processing UNA.AS^K20... ✓ 323 rows\n",
      "[287/411] Processing UPM.HE... ✓ 1487 rows\n",
      "[288/411] Processing URW.PA... ✓ 1517 rows\n",
      "[289/411] Processing VALMT.HE... ✓ 1487 rows\n",
      "[290/411] Processing VCTP.PA... ✓ 1517 rows\n",
      "[291/411] Processing VENDA.OL... ⚠ Volume data failed: Error code -1 | No data to return, please check errors: ERROR: No successful response.\n",
      "(TS.Interday.UserRequestError.70005, The universe is not found), continuing with fundamentals... ✓ 2162 rows\n",
      "[292/411] Processing VIE.PA... ✓ 1517 rows\n",
      "[293/411] Processing VIRB.PA... ✓ 1517 rows\n",
      "[294/411] Processing VIRI.PA... ✓ 1517 rows\n",
      "[295/411] Processing VIV.PA... ✓ 1517 rows\n",
      "[296/411] Processing VLLP.PA... ✓ 1517 rows\n",
      "[297/411] Processing VLOF.PA... ✓ 1517 rows\n",
      "[298/411] Processing VNAn.DE... ✓ 1506 rows\n",
      "[299/411] Processing VOLVb.ST... ✓ 1488 rows\n",
      "[300/411] Processing VOPA.AS... ✓ 1517 rows\n",
      "[301/411] Processing VOWG_p.DE... ✓ 1506 rows\n",
      "[302/411] Processing VRLA.PA... ✓ 1494 rows\n",
      "[303/411] Processing VWS.CO... ✓ 1481 rows\n",
      "[304/411] Processing WDIG.DE^A21... ✓ 340 rows\n",
      "[305/411] Processing WDPP.BR... ✓ 1517 rows\n",
      "[306/411] Processing WEHA.AS... ✓ 1517 rows\n",
      "[307/411] Processing WLN.PA... ✓ 1517 rows\n",
      "[308/411] Processing WLSNc.AS... ✓ 1517 rows\n",
      "[309/411] Processing WRT1V.HE... ✓ 1487 rows\n",
      "[310/411] Processing YAR.OL... ✓ 1487 rows\n",
      "[311/411] Processing ABIO.PA^J22... ✓ 803 rows\n",
      "[312/411] Processing ALMCP.PA... ✓ 1517 rows\n",
      "[313/411] Processing AOO.BR... ✓ 1517 rows\n",
      "[314/411] Processing CARM.PA... ✓ 1517 rows\n",
      "[315/411] Processing DHER.DE... ✓ 1506 rows\n",
      "[316/411] Processing DWNG.DE... ✓ 1506 rows\n",
      "[317/411] Processing ENTRA.OL... ✓ 1487 rows\n",
      "[318/411] Processing FDJU.PA... ✓ 1461 rows\n",
      "[319/411] Processing JDEP.AS... ✓ 1329 rows\n",
      "[320/411] Processing KOF.PA... ✓ 1517 rows\n",
      "[321/411] Processing KOJAMO.HE... ✓ 1487 rows\n",
      "[322/411] Processing NEOEN.PA^D25... ✓ 1434 rows\n",
      "[323/411] Processing NSTEc.AS... ✓ 1517 rows\n",
      "[324/411] Processing PHAR.AS... ✓ 1517 rows\n",
      "[325/411] Processing ROBF.PA... ✓ 1517 rows\n",
      "[326/411] Processing ROCKb.CO... ✓ 1481 rows\n",
      "[327/411] Processing S30.PA... ✓ 1517 rows\n",
      "[328/411] Processing SCATC.OL... ✓ 1487 rows\n",
      "[329/411] Processing ULVR.AS... ✓ 1517 rows\n",
      "[330/411] Processing ADEA.OL^F24... ✓ 1195 rows\n",
      "[331/411] Processing AIRG.DE... ✓ 1506 rows\n",
      "[332/411] Processing ALFEN.AS... ✓ 1517 rows\n",
      "[333/411] Processing BNRGn.DE... ✓ 1506 rows\n",
      "[334/411] Processing DBG.PA... ✓ 1517 rows\n",
      "[335/411] Processing ELI.BR... ✓ 1517 rows\n",
      "[336/411] Processing ENR1n.DE... ✓ 1236 rows\n",
      "[337/411] Processing EVOG.ST... ✓ 1488 rows\n",
      "[338/411] Processing HFGG.DE... ✓ 1506 rows\n",
      "[339/411] Processing INPST.AS... ✓ 1159 rows\n",
      "[340/411] Processing KAHOT.OL^A24... ✓ 1078 rows\n",
      "[341/411] Processing MAERSKa.CO... ✓ 1481 rows\n",
      "[342/411] Processing MLXS.BR... ✓ 1517 rows\n",
      "[343/411] Processing MPCC.OL... ✓ 1487 rows\n",
      "[344/411] Processing NETCG.CO... ✓ 1481 rows\n",
      "[345/411] Processing NOD.OL... ✓ 1487 rows\n",
      "[346/411] Processing OVH.PA... ✓ 974 rows\n",
      "[347/411] Processing PSHG_p.DE... ✓ 1506 rows\n",
      "[348/411] Processing PUMG.DE... ✓ 1506 rows\n",
      "[349/411] Processing QIA.DE... ✓ 1506 rows\n",
      "[350/411] Processing QTCOM.HE... ✓ 1487 rows\n",
      "[351/411] Processing RECSI.OL... ✓ 1487 rows\n",
      "[352/411] Processing SATG_p.DE... ✓ 1506 rows\n",
      "[353/411] Processing SHLG.DE... ✓ 1506 rows\n",
      "[354/411] Processing SINCH.ST... ✓ 1488 rows\n",
      "[355/411] Processing STLAM.PA... ✓ 1517 rows\n",
      "[356/411] Processing SY1G.DE... ✓ 1506 rows\n",
      "[357/411] Processing TE.PA... ✓ 1145 rows\n",
      "[358/411] Processing UMG.AS... ✓ 992 rows\n",
      "[359/411] Processing VLS.PA... ✓ 1517 rows\n",
      "[360/411] Processing ZALG.DE... ✓ 1506 rows\n",
      "[361/411] Processing ANTIN.PA... ✓ 989 rows\n",
      "[362/411] Processing AUTO.OL... ✓ 952 rows\n",
      "[363/411] Processing BAVA.CO... ✓ 1481 rows\n",
      "[364/411] Processing CTPNV.AS... ✓ 1118 rows\n",
      "[365/411] Processing DAMA.PA^B23... ✓ 886 rows\n",
      "[366/411] Processing DTGGe.DE... ✓ 929 rows\n",
      "[367/411] Processing EAPI.PA... ✓ 831 rows\n",
      "[368/411] Processing EXOR.AS... ✓ 761 rows\n",
      "[369/411] Processing HNRGn.DE... ✓ 1506 rows\n",
      "[370/411] Processing IETB.BR... ✓ 1517 rows\n",
      "[371/411] Processing IMAF.PA... ✓ 1517 rows\n",
      "[372/411] Processing IPAR.PA... ✓ 1517 rows\n",
      "[373/411] Processing JYSK.CO... ✓ 1481 rows\n",
      "[374/411] Processing KOG.OL... ✓ 1487 rows\n",
      "[375/411] Processing P911_p.DE... ✓ 724 rows\n",
      "[376/411] Processing PGS.OL^G24... ✓ 1214 rows\n",
      "[377/411] Processing SBBb.ST... ✓ 1488 rows\n",
      "[378/411] Processing SSABBH.HE... ✓ 1487 rows\n",
      "[379/411] Processing TOKMAN.HE... ✓ 1487 rows\n",
      "[380/411] Processing VAR.OL... ✓ 869 rows\n",
      "[381/411] Processing VGP1.BR... ✓ 1517 rows\n",
      "[382/411] Processing VU.PA... ✓ 1517 rows\n",
      "[383/411] Processing ALLFG.AS... ✓ 1099 rows\n",
      "[384/411] Processing ARGAN.PA... ✓ 1517 rows\n",
      "[385/411] Processing BORR.OL^A25... ✓ 1341 rows\n",
      "[386/411] Processing CBKG.DE... ✓ 1506 rows\n",
      "[387/411] Processing CBLP.PA... ✓ 1517 rows\n",
      "[388/411] Processing CHBE.PA... ✓ 1517 rows\n",
      "[389/411] Processing DSFIR.AS... ✓ 586 rows\n",
      "[390/411] Processing HAFNI.OL... ✓ 1438 rows\n",
      "[391/411] Processing HAUTO.OL... ✓ 924 rows\n",
      "[392/411] Processing IDLA.PA... ✓ 1517 rows\n",
      "[393/411] Processing LECS.PA... ✓ 1517 rows\n",
      "[394/411] Processing NIBEb.ST... ✓ 1488 rows\n",
      "[395/411] Processing RHMG.DE... ✓ 1506 rows\n",
      "[396/411] Processing SYENS.BR... ✓ 418 rows\n",
      "[397/411] Processing VLAN.AS... ✓ 1517 rows\n",
      "[398/411] Processing VLTSA.PA... ✓ 1517 rows\n",
      "[399/411] Processing XFAB.PA... ✓ 1517 rows\n",
      "[400/411] Processing AZE.BR... ✓ 994 rows\n",
      "[401/411] Processing ESSF.PA... ✓ 1517 rows\n",
      "[402/411] Processing LOTB.BR... ✓ 1517 rows\n",
      "[403/411] Processing MANTA.HE... ✓ 459 rows\n",
      "[404/411] Processing MAUP.PA... ✓ 1517 rows\n",
      "[405/411] Processing MEDCL.PA... ✓ 1517 rows\n",
      "[406/411] Processing NKT.CO... ✓ 1481 rows\n",
      "[407/411] Processing PLNW.PA... ✓ 461 rows\n",
      "[408/411] Processing PLX.PA... ✓ 384 rows\n",
      "[409/411] Processing SAABb.ST... ✓ 1488 rows\n",
      "[410/411] Processing WAWI.OL... ✓ 1487 rows\n",
      "[411/411] Processing ZELA.CO... ✓ 1481 rows\n",
      "\n",
      "================================================================================\n",
      "PROCESSING COMPLETE\n",
      "================================================================================\n",
      "Successfully processed: 411 stocks\n",
      "Failed: 0 stocks\n",
      "\n",
      "Combined dataset shape: (583515, 4)\n",
      "Date range: 2019-09-01 00:00:00 to 2025-08-01 00:00:00\n",
      "Unique stocks: 411\n",
      "\n",
      "Sample data:\n",
      "        RIC       Date      volume   cap\n",
      "0  1COVG.DE 2019-09-02  23268570.0  <NA>\n",
      "1  1COVG.DE 2019-09-03  31323010.0  <NA>\n",
      "2  1COVG.DE 2019-09-04  29976230.0  <NA>\n",
      "3  1COVG.DE 2019-09-05  40698920.0  <NA>\n",
      "4  1COVG.DE 2019-09-06  31819200.0  <NA>\n",
      "5  1COVG.DE 2019-09-09  57319860.0  <NA>\n",
      "6  1COVG.DE 2019-09-10  76464850.0  <NA>\n",
      "7  1COVG.DE 2019-09-11  45576000.0  <NA>\n",
      "8  1COVG.DE 2019-09-12  67230330.0  <NA>\n",
      "9  1COVG.DE 2019-09-13  72806550.0  <NA>\n",
      "\n",
      "✓ Saved to extras.csv\n",
      "\n",
      "Null counts by column:\n",
      "RIC           0\n",
      "Date          0\n",
      "volume     2636\n",
      "cap       41109\n",
      "dtype: int64\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "rd.open_session()\n",
    "\n",
    "unique_rics = universe['RIC'].unique()\n",
    "print(f\"Total rows in universe: {len(universe)}\")\n",
    "print(f\"Unique RICs to process: {len(unique_rics)}\")\n",
    "\n",
    "start_date = \"2019-09-01\"\n",
    "end_date = \"2025-08-01\"\n",
    "\n",
    "all_data = []\n",
    "failed_rics = []\n",
    "\n",
    "total_rics = len(unique_rics)\n",
    "print(f\"\\nProcessing {total_rics} unique stocks...\")\n",
    "\n",
    "for idx, ric in enumerate(unique_rics, 1):\n",
    "    print(f\"[{idx}/{total_rics}] Processing {ric}...\", end=' ')\n",
    "    \n",
    "    try:\n",
    "        # --- Fetch historical PRICING data (for volume) ---\n",
    "        df = None\n",
    "        try:\n",
    "            price_df = rd.get_history(\n",
    "                universe=ric,\n",
    "                start=start_date,\n",
    "                end=end_date,\n",
    "                interval=\"daily\"\n",
    "            )\n",
    "            \n",
    "            # Extract only volume\n",
    "            df = pd.DataFrame({\n",
    "                'RIC': ric,\n",
    "                'volume': price_df['TRNOVR_UNS'] if 'TRNOVR_UNS' in price_df.columns else None\n",
    "            })\n",
    "        except Exception as price_error:\n",
    "            print(f\"⚠ Volume data failed: {price_error}, continuing with fundamentals...\", end=' ')\n",
    "            # Create empty DataFrame with date range if price fetch fails\n",
    "            date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "            df = pd.DataFrame({\n",
    "                'RIC': ric,\n",
    "                'volume': None\n",
    "            }, index=date_range)\n",
    "        \n",
    "        # --- Fetch FUNDAMENTAL data (Market Cap and Book-to-Market) ---\n",
    "        fundamental_response = rd.get_data(\n",
    "            universe=ric,\n",
    "            fields=[\n",
    "                \"TR.PriceToBVPerShare.Date\",\n",
    "                \"TR.PriceToBVPerShare\",  # Book-to-Market Value\n",
    "                \"TR.CompanyMarketCap\"\n",
    "            ],\n",
    "            parameters={\n",
    "                \"SDate\": start_date,\n",
    "                \"EDate\": end_date,\n",
    "                \"Frq\": \"FY\",  # Financial Year frequency\n",
    "                \"Curn\": \"EUR\"  # Currency in Euros\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Process fundamental data\n",
    "        if fundamental_response is not None and isinstance(fundamental_response, pd.DataFrame):\n",
    "            fund_df = fundamental_response.copy()\n",
    "            \n",
    "            if 'Date' in fund_df.columns:\n",
    "                fund_df['Date'] = pd.to_datetime(fund_df['Date'])\n",
    "                fund_df.set_index('Date', inplace=True)\n",
    "                \n",
    "                if 'Instrument' in fund_df.columns:\n",
    "                    fund_df.drop('Instrument', axis=1, inplace=True)\n",
    "                \n",
    "                # Merge with volume data\n",
    "                df = df.join(fund_df, how='left')\n",
    "        \n",
    "        # Rename columns\n",
    "        column_mapping = {\n",
    "            'Price to Book Value Per Share': 'mtbv',\n",
    "            'Company Market Cap': 'cap'\n",
    "        }\n",
    "        \n",
    "        for old_name, new_name in column_mapping.items():\n",
    "            if old_name in df.columns:\n",
    "                df.rename(columns={old_name: new_name}, inplace=True)\n",
    "        \n",
    "        # Forward-fill mtbv and cap to have daily values\n",
    "        if 'mtbv' in df.columns:\n",
    "            df['mtbv'] = df['mtbv'].ffill()\n",
    "        if 'cap' in df.columns:\n",
    "            df['cap'] = df['cap'].ffill()\n",
    "        \n",
    "        # Reset index to make Date a column\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={'index': 'Date'}, inplace=True)\n",
    "        \n",
    "        # Reorder columns\n",
    "        desired_order = ['RIC', 'Date', 'volume', 'mtbv', 'cap']\n",
    "        existing_cols = [col for col in desired_order if col in df.columns]\n",
    "        df = df[existing_cols]\n",
    "        \n",
    "        # Append to list\n",
    "        all_data.append(df)\n",
    "        \n",
    "        print(f\"✓ {len(df)} rows\")\n",
    "        \n",
    "        # Rate limiting - sleep briefly to avoid overwhelming the API\n",
    "        if idx % 10 == 0:\n",
    "            time.sleep(2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed: {e}\")\n",
    "        failed_rics.append(ric)\n",
    "        continue\n",
    "\n",
    "# --- Combine all data ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Successfully processed: {len(all_data)} stocks\")\n",
    "print(f\"Failed: {len(failed_rics)} stocks\")\n",
    "\n",
    "if failed_rics:\n",
    "    print(f\"\\nFailed RICs:\")\n",
    "    for ric in failed_rics:\n",
    "        print(f\"  - {ric}\")\n",
    "\n",
    "# --- Create combined DataFrame ---\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nCombined dataset shape: {combined_df.shape}\")\n",
    "    print(f\"Date range: {combined_df['Date'].min()} to {combined_df['Date'].max()}\")\n",
    "    print(f\"Unique stocks: {combined_df['RIC'].nunique()}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample data:\")\n",
    "    print(combined_df.head(10))\n",
    "    \n",
    "    # Save to CSV\n",
    "    combined_df.to_csv(\"extras.csv\", index=False)\n",
    "    print(f\"\\n✓ Saved to extras.csv\")\n",
    "    \n",
    "    # Show null counts\n",
    "    print(\"\\nNull counts by column:\")\n",
    "    print(combined_df.isnull().sum())\n",
    "else:\n",
    "    print(\"\\nNo data retrieved!\")\n",
    "\n",
    "# --- Close session ---\n",
    "rd.close_session()\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22f222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
