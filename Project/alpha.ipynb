{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2a90e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba379bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = pd.read_csv(\"sentiment.csv\", index_col=0, parse_dates=True)\n",
    "rec  = pd.read_csv(\"recommendation.csv\", index_col=0, parse_dates=True)\n",
    "price = pd.read_csv(\"price.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "sent, rec = sent.align(rec, join=\"inner\", axis=0)\n",
    "sent, price = sent.align(price, join=\"inner\", axis=0)\n",
    "rec = rec.loc[sent.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c6d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment features - multiple windows and transformations\n",
    "f_sent1 = sent.add_suffix(\"_s1\")\n",
    "f_sent3 = sent.rolling(3).mean().add_suffix(\"_s3\")\n",
    "f_sent7 = sent.rolling(7).mean().add_suffix(\"_s7\")\n",
    "f_sent14 = sent.rolling(14).mean().add_suffix(\"_s14\")\n",
    "f_sent_std = sent.rolling(7).std().add_suffix(\"_s_std\")  # Sentiment volatility\n",
    "f_sent_change = sent.diff().add_suffix(\"_s_change\")  # Change in sentiment\n",
    "\n",
    "# Recommendation features\n",
    "f_rec1 = rec.add_suffix(\"_r1\")\n",
    "f_rec3 = rec.rolling(3).mean().add_suffix(\"_r3\")\n",
    "f_rec7 = rec.rolling(7).mean().add_suffix(\"_r7\")\n",
    "f_rec_change = rec.diff().add_suffix(\"_r_change\")  # Change in recommendations\n",
    "\n",
    "# Price-based momentum features\n",
    "returns = price.pct_change(fill_method=None)\n",
    "mom1 = returns.add_suffix(\"_mom1\")\n",
    "mom5 = price.pct_change(5, fill_method=None).add_suffix(\"_mom5\")\n",
    "mom10 = price.pct_change(10, fill_method=None).add_suffix(\"_mom10\")\n",
    "mom21 = price.pct_change(21, fill_method=None).add_suffix(\"_mom21\")\n",
    "mom63 = price.pct_change(63, fill_method=None).add_suffix(\"_mom63\")  # 3-month momentum\n",
    "\n",
    "# Volatility features\n",
    "vol5 = returns.rolling(5).std().add_suffix(\"_vol5\")\n",
    "vol10 = returns.rolling(10).std().add_suffix(\"_vol10\")\n",
    "vol21 = returns.rolling(21).std().add_suffix(\"_vol21\")\n",
    "\n",
    "# RSI-like features (relative strength)\n",
    "def rsi_like(series, window=14):\n",
    "    \"\"\"RSI-like indicator: (avg gain - avg loss) / (avg gain + avg loss)\"\"\"\n",
    "    delta = series.pct_change(fill_method=None)\n",
    "    gain = delta.where(delta > 0, 0).rolling(window).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    return (rs - 1) / (rs + 1)\n",
    "\n",
    "rsi14 = price.apply(lambda x: rsi_like(x, 14)).add_suffix(\"_rsi14\")\n",
    "rsi7 = price.apply(lambda x: rsi_like(x, 7)).add_suffix(\"_rsi7\")\n",
    "\n",
    "# Mean reversion features\n",
    "price_ma5 = price.rolling(5).mean()\n",
    "price_ma21 = price.rolling(21).mean()\n",
    "price_ma63 = price.rolling(63).mean()\n",
    "zscore5 = ((price - price_ma5) / (price.rolling(5).std() + 1e-10)).add_suffix(\"_zscore5\")\n",
    "zscore21 = ((price - price_ma21) / (price.rolling(21).std() + 1e-10)).add_suffix(\"_zscore21\")\n",
    "\n",
    "# Relative features (stock vs market)\n",
    "market_sent = sent.mean(axis=1)\n",
    "market_rec = rec.mean(axis=1)\n",
    "market_ret = returns.mean(axis=1)\n",
    "\n",
    "sent_rel = sent.sub(market_sent, axis=0).add_suffix(\"_sent_rel\")\n",
    "rec_rel = rec.sub(market_rec, axis=0).add_suffix(\"_rec_rel\")\n",
    "ret_rel = returns.sub(market_ret, axis=0).add_suffix(\"_ret_rel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848bf6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (270, 1268)\n",
      "Features with NaN: 55471\n",
      "Features non-NaN count: 286889\n",
      "Sample feature columns: ['1COVG.DE_s1', 'AALB.AS_s1', 'ABB.ST_s1', 'ABI.BR_s1', 'ABNd.AS_s1']\n"
     ]
    }
   ],
   "source": [
    "# Combine all features\n",
    "features = pd.concat([\n",
    "    f_sent1, f_sent3, f_sent7, f_sent14, f_sent_std, f_sent_change,\n",
    "    f_rec1, f_rec3, f_rec7, f_rec_change,\n",
    "    mom1, mom5, mom10, mom21, mom63,\n",
    "    vol5, vol10, vol21,\n",
    "    rsi7, rsi14,\n",
    "    zscore5, zscore21,\n",
    "    sent_rel, rec_rel, ret_rel\n",
    "], axis=1)\n",
    "\n",
    "# Lag all features by 1 day to avoid look-ahead bias\n",
    "# This ensures we use yesterday's features to predict tomorrow's return\n",
    "features_lagged = features.shift(1)\n",
    "\n",
    "# Target: next day's return\n",
    "returns = price.pct_change(fill_method=None).shift(-1)\n",
    "\n",
    "# Debug: Check feature statistics\n",
    "print(f\"Features shape: {features_lagged.shape}\")\n",
    "print(f\"Features with NaN: {features_lagged.isna().sum().sum()}\")\n",
    "print(f\"Features non-NaN count: {features_lagged.notna().sum().sum()}\")\n",
    "print(f\"Sample feature columns: {list(features_lagged.columns[:5])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acdffa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After joining and dropping NaN:\n",
      "DataFrame shape: (6491, 26)\n",
      "Target stats: mean=0.000304, std=0.028934\n",
      "Target NaN count: 0\n",
      "Feature columns: 25\n",
      "\n",
      "After outlier removal:\n",
      "DataFrame shape: (5705, 26)\n",
      "Remaining samples: 5705\n",
      "\n",
      "Final X_mat shape: (5705, 25)\n",
      "X_mat NaN count: 0\n",
      "X_mat constant columns: 1\n",
      "y_vec NaN count: 0\n",
      "y_vec stats: mean=0.000361, std=0.022659\n"
     ]
    }
   ],
   "source": [
    "# Reshape features for modeling\n",
    "X = features_lagged.stack().reset_index()\n",
    "X.columns = [\"date\", \"col\", \"value\"]\n",
    "\n",
    "# Extract RIC and feature name from column names\n",
    "# Handle multi-part feature names (e.g., \"AAPL_sent_rel\")\n",
    "X[\"ric\"] = X[\"col\"].str.split(\"_\").str[0]\n",
    "# Join remaining parts as feature name\n",
    "X[\"feature_name\"] = X[\"col\"].str.split(\"_\", n=1).str[1]\n",
    "\n",
    "X = X.drop(columns=[\"col\"])\n",
    "\n",
    "X = X.pivot_table(index=[\"date\",\"ric\"], \n",
    "                  columns=\"feature_name\", \n",
    "                  values=\"value\")\n",
    "\n",
    "y = returns.stack().reset_index()\n",
    "y.columns = [\"date\", \"ric\", \"target\"]\n",
    "y = y.set_index([\"date\",\"ric\"])\n",
    "\n",
    "# Align X and y on index before joining\n",
    "df = X.join(y, how=\"inner\").dropna()\n",
    "\n",
    "print(f\"\\nAfter joining and dropping NaN:\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Target stats: mean={df['target'].mean():.6f}, std={df['target'].std():.6f}\")\n",
    "print(f\"Target NaN count: {df['target'].isna().sum()}\")\n",
    "print(f\"Feature columns: {len([c for c in df.columns if c != 'target'])}\")\n",
    "\n",
    "# Remove extreme outliers (beyond 3 standard deviations) - more efficient approach\n",
    "feature_cols = [col for col in df.columns if col != \"target\"]\n",
    "mask = pd.Series(True, index=df.index)\n",
    "\n",
    "for col in feature_cols:\n",
    "    mean = df[col].mean()\n",
    "    std = df[col].std()\n",
    "    if std > 0:  # Avoid division by zero\n",
    "        mask = mask & (df[col] >= mean - 3*std) & (df[col] <= mean + 3*std)\n",
    "\n",
    "df = df[mask]\n",
    "\n",
    "print(f\"\\nAfter outlier removal:\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Remaining samples: {len(df)}\")\n",
    "\n",
    "X_mat = df.drop(columns=[\"target\"])\n",
    "y_vec = df[\"target\"]\n",
    "\n",
    "# Final check\n",
    "print(f\"\\nFinal X_mat shape: {X_mat.shape}\")\n",
    "print(f\"X_mat NaN count: {X_mat.isna().sum().sum()}\")\n",
    "print(f\"X_mat constant columns: {(X_mat.nunique() == 1).sum()}\")\n",
    "print(f\"y_vec NaN count: {y_vec.isna().sum()}\")\n",
    "print(f\"y_vec stats: mean={y_vec.mean():.6f}, std={y_vec.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea1f661",
   "metadata": {},
   "source": [
    "## LASSO REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7fad095",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LassoCV(cv=5, n_alphas=100)\n",
    "model.fit(X_mat, y_vec)\n",
    "\n",
    "pred = model.predict(X_mat)\n",
    "df[\"predicted_return\"] = pred\n",
    "\n",
    "market_ret = returns.mean(axis=1)\n",
    "df[\"market\"] = df.index.get_level_values(\"date\").map(market_ret)\n",
    "df[\"alpha\"] = df[\"predicted_return\"] - df[\"market\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6935656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC: 0.01508442677701171\n"
     ]
    }
   ],
   "source": [
    "ic = np.corrcoef(df[\"predicted_return\"], df[\"target\"])[0,1]\n",
    "print(\"IC:\", ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7280b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directional accuracy: 0.5123575810692375\n"
     ]
    }
   ],
   "source": [
    "accuracy = ((df[\"predicted_return\"] > 0) \n",
    "            == (df[\"target\"] > 0)).mean()\n",
    "\n",
    "print(\"Directional accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe84d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ace67c55",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06447ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBOOST with improved features and hyperparameters...\n",
      "------------------------------------------------------------\n",
      "Fold 1 IC: NaN (test_size: 950, pred_std: 0.000000, target_std: 0.028672, valid: 950)\n",
      "Fold 2 IC: NaN (test_size: 950, pred_std: 0.000000, target_std: 0.020184, valid: 950)\n",
      "Fold 3 IC: NaN (test_size: 950, pred_std: 0.000000, target_std: 0.023320, valid: 950)\n",
      "Fold 4 IC: NaN (test_size: 950, pred_std: 0.000000, target_std: 0.019372, valid: 950)\n",
      "Fold 5 IC: NaN (test_size: 950, pred_std: 0.000000, target_std: 0.022443, valid: 950)\n",
      "------------------------------------------------------------\n",
      "Overall IC out-of-sample: -0.035068\n",
      "Mean fold IC: NaN (all folds had NaN IC)\n",
      "Directional accuracy: 0.4920\n",
      "Rank IC (Spearman): -0.042846\n",
      "\n",
      "Predictions stats: mean=0.000418, std=0.000480\n",
      "Actuals stats: mean=0.000057, std=0.023148\n",
      "Unique predictions: 5\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "preds = []\n",
    "actuals = []\n",
    "fold_ics = []\n",
    "\n",
    "print(\"Training XGBOOST with improved features and hyperparameters...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X_mat), 1):\n",
    "    X_train, X_test = X_mat.iloc[train_idx], X_mat.iloc[test_idx]\n",
    "    y_train, y_test = y_vec.iloc[train_idx], y_vec.iloc[test_idx]\n",
    "    \n",
    "    # Check for issues\n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(f\"Fold {fold}: Empty train or test set!\")\n",
    "        continue\n",
    "    \n",
    "    # Remove constant features\n",
    "    constant_cols = X_train.columns[X_train.nunique() == 1]\n",
    "    if len(constant_cols) > 0:\n",
    "        X_train = X_train.drop(columns=constant_cols)\n",
    "        X_test = X_test.drop(columns=constant_cols)\n",
    "    \n",
    "    if X_train.shape[1] == 0:\n",
    "        print(f\"Fold {fold}: No valid features after removing constants!\")\n",
    "        continue\n",
    "    \n",
    "    # Further split training data for validation\n",
    "    split_point = int(len(X_train) * 0.9)\n",
    "    if split_point == 0:\n",
    "        split_point = len(X_train)\n",
    "    X_train_fit, X_val = X_train.iloc[:split_point], X_train.iloc[split_point:]\n",
    "    y_train_fit, y_val = y_train.iloc[:split_point], y_train.iloc[split_point:]\n",
    "\n",
    "    # Use a reasonable fixed number of estimators with strong regularization\n",
    "    # This avoids early stopping compatibility issues across XGBoost versions\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=500,  # Fixed number - regularization prevents overfitting\n",
    "        max_depth=5,  # Slightly deeper for more complex patterns\n",
    "        learning_rate=0.01,  # Lower learning rate for better generalization\n",
    "        subsample=0.85,  # Slightly higher subsample\n",
    "        colsample_bytree=0.85,  # Feature subsampling\n",
    "        colsample_bylevel=0.85,  # Additional regularization\n",
    "        min_child_weight=3,  # Regularization\n",
    "        gamma=0.1,  # Minimum loss reduction for splits\n",
    "        reg_alpha=0.1,  # L1 regularization\n",
    "        reg_lambda=1.0,  # L2 regularization\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit model - use validation set for monitoring but don't rely on early stopping\n",
    "    try:\n",
    "        if len(X_val) > 0:\n",
    "            xgb.fit(\n",
    "                X_train_fit, y_train_fit,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            xgb.fit(X_train_fit, y_train_fit, verbose=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Fold {fold}: Error fitting model: {e}\")\n",
    "        continue\n",
    "    \n",
    "    best_iter = 500  # Fixed number of estimators used\n",
    "    \n",
    "    fold_preds = xgb.predict(X_test)\n",
    "    preds.extend(fold_preds)\n",
    "    actuals.extend(y_test)\n",
    "    \n",
    "    # Calculate IC with robust NaN handling\n",
    "    fold_preds_array = np.array(fold_preds)\n",
    "    y_test_array = np.array(y_test)\n",
    "    \n",
    "    # Remove any NaN values\n",
    "    valid_mask = ~(np.isnan(fold_preds_array) | np.isnan(y_test_array))\n",
    "    pred_std = 0.0\n",
    "    target_std = 0.0\n",
    "    \n",
    "    if valid_mask.sum() < 2:\n",
    "        fold_ic = np.nan\n",
    "        fold_ic_spearman = np.nan\n",
    "    else:\n",
    "        fold_preds_clean = fold_preds_array[valid_mask]\n",
    "        y_test_clean = y_test_array[valid_mask]\n",
    "        \n",
    "        # Check for sufficient variance\n",
    "        pred_std = np.std(fold_preds_clean)\n",
    "        target_std = np.std(y_test_clean)\n",
    "        \n",
    "        if pred_std > 1e-10 and target_std > 1e-10 and len(fold_preds_clean) > 1:\n",
    "            try:\n",
    "                # Try Pearson correlation first\n",
    "                fold_ic = np.corrcoef(fold_preds_clean, y_test_clean)[0, 1]\n",
    "                if np.isnan(fold_ic) or np.isinf(fold_ic):\n",
    "                    # Fall back to Spearman\n",
    "                    fold_ic = spearmanr(fold_preds_clean, y_test_clean)[0]\n",
    "                    fold_ic_spearman = fold_ic\n",
    "                else:\n",
    "                    # Also calculate Spearman for comparison\n",
    "                    fold_ic_spearman = spearmanr(fold_preds_clean, y_test_clean)[0]\n",
    "            except Exception as e:\n",
    "                fold_ic = np.nan\n",
    "                fold_ic_spearman = np.nan\n",
    "        else:\n",
    "            fold_ic = np.nan\n",
    "            fold_ic_spearman = np.nan\n",
    "    \n",
    "    fold_ics.append(fold_ic)\n",
    "    \n",
    "    # Print detailed fold information\n",
    "    if not np.isnan(fold_ic):\n",
    "        print(f\"Fold {fold} IC: {fold_ic:.6f} (Spearman: {fold_ic_spearman:.6f}, test_size: {len(X_test)}, valid: {valid_mask.sum()})\")\n",
    "    else:\n",
    "        print(f\"Fold {fold} IC: NaN (test_size: {len(X_test)}, pred_std: {pred_std:.6f}, target_std: {target_std:.6f}, valid: {valid_mask.sum()})\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "if len(preds) > 1 and np.std(preds) > 0 and np.std(actuals) > 0:\n",
    "    ic_oos = np.corrcoef(preds, actuals)[0, 1]\n",
    "    if np.isnan(ic_oos):\n",
    "        ic_oos = spearmanr(preds, actuals)[0]\n",
    "else:\n",
    "    ic_oos = np.nan\n",
    "\n",
    "print(f\"Overall IC out-of-sample: {ic_oos:.6f}\")\n",
    "valid_fold_ics = [ic for ic in fold_ics if not np.isnan(ic)]\n",
    "if len(valid_fold_ics) > 0:\n",
    "    print(f\"Mean fold IC: {np.mean(valid_fold_ics):.6f}\")\n",
    "    print(f\"Std fold IC: {np.std(valid_fold_ics):.6f}\")\n",
    "else:\n",
    "    print(\"Mean fold IC: NaN (all folds had NaN IC)\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "preds_array = np.array(preds)\n",
    "actuals_array = np.array(actuals)\n",
    "if len(preds_array) > 0:\n",
    "    directional_accuracy = ((preds_array > 0) == (actuals_array > 0)).mean()\n",
    "    print(f\"Directional accuracy: {directional_accuracy:.4f}\")\n",
    "    \n",
    "    # Rank IC (correlation of ranks)\n",
    "    if len(preds_array) > 1:\n",
    "        rank_ic = spearmanr(preds_array, actuals_array)[0]\n",
    "        print(f\"Rank IC (Spearman): {rank_ic:.6f}\")\n",
    "    \n",
    "    # Debug info\n",
    "    print(f\"\\nPredictions stats: mean={np.mean(preds_array):.6f}, std={np.std(preds_array):.6f}\")\n",
    "    print(f\"Actuals stats: mean={np.mean(actuals_array):.6f}, std={np.std(actuals_array):.6f}\")\n",
    "    print(f\"Unique predictions: {len(np.unique(preds_array))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8649b34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1 constant features for feature importance analysis\n",
      "\n",
      "Top 20 Most Important Features:\n",
      "============================================================\n",
      " feature  importance\n",
      "    mom1         0.0\n",
      "   mom10         0.0\n",
      "zscore21         0.0\n",
      "    vol5         0.0\n",
      "   vol21         0.0\n",
      "   vol10         0.0\n",
      "sent_rel         0.0\n",
      "   s_std         0.0\n",
      "s_change         0.0\n",
      "      s7         0.0\n",
      "      s3         0.0\n",
      "     s14         0.0\n",
      "      s1         0.0\n",
      "    rsi7         0.0\n",
      "   rsi14         0.0\n",
      " ret_rel         0.0\n",
      " rec_rel         0.0\n",
      "      r7         0.0\n",
      "      r3         0.0\n",
      "      r1         0.0\n",
      "\n",
      "Features with zero importance: 24\n"
     ]
    }
   ],
   "source": [
    "# Feature importance analysis\n",
    "# Train a final model on all data (except last fold) to get feature importance\n",
    "final_train_idx = list(tscv.split(X_mat))[-1][0]  # Use all but last fold for training\n",
    "X_train_final = X_mat.iloc[final_train_idx].copy()\n",
    "y_train_final = y_vec.iloc[final_train_idx].copy()\n",
    "\n",
    "# Remove constant features\n",
    "constant_cols = X_train_final.columns[X_train_final.nunique() == 1]\n",
    "if len(constant_cols) > 0:\n",
    "    print(f\"Removing {len(constant_cols)} constant features for feature importance analysis\")\n",
    "    X_train_final = X_train_final.drop(columns=constant_cols)\n",
    "\n",
    "if X_train_final.shape[1] == 0:\n",
    "    print(\"No valid features for feature importance analysis!\")\n",
    "else:\n",
    "    split_point = int(len(X_train_final) * 0.9)\n",
    "    if split_point == 0:\n",
    "        split_point = len(X_train_final)\n",
    "    X_train_fit_final, X_val_final = X_train_final.iloc[:split_point], X_train_final.iloc[split_point:]\n",
    "    y_train_fit_final, y_val_final = y_train_final.iloc[:split_point], y_train_final.iloc[split_point:]\n",
    "\n",
    "    xgb_final = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.01,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        colsample_bylevel=0.85,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.1,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit model\n",
    "    try:\n",
    "        if len(X_val_final) > 0:\n",
    "            xgb_final.fit(\n",
    "                X_train_fit_final, y_train_fit_final,\n",
    "                eval_set=[(X_val_final, y_val_final)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            xgb_final.fit(X_train_fit_final, y_train_fit_final, verbose=False)\n",
    "        \n",
    "        # Get feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X_train_final.columns,\n",
    "            'importance': xgb_final.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        print(\"\\nTop 20 Most Important Features:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(feature_importance.head(20).to_string(index=False))\n",
    "        \n",
    "        # Show features with zero importance\n",
    "        zero_importance = feature_importance[feature_importance['importance'] == 0]\n",
    "        if len(zero_importance) > 0:\n",
    "            print(f\"\\nFeatures with zero importance: {len(zero_importance)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature importance analysis: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c7253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
